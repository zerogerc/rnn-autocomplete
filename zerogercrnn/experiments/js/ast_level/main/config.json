{
  "prediction_type": "nt_seq_sum_attention",
  "non_terminals_count": 97,
  "terminals_count": 50002,
  "data_train_limit": 100000,
  "data_eval_limit": 50000,
  "seq_len": 10,
  "batch_size": 40,
  "learning_rate": 0.005,
  "epochs": 60,
  "decay_after_epoch": 10,
  "decay_multiplier": 0.9,
  "embedding_size": 50,
  "hidden_size": 1500,
  "num_layers": 1,
  "dropout": 0.05,
  "weight_decay": 0.01
}